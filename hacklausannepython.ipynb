{"nbformat_minor": 0, "metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 2}, "version": "2.7.11", "pygments_lexer": "ipython2", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python2-spark20", "language": "python", "display_name": "Python 2 with Spark 2.0"}}, "cells": [{"source": "\n# @hidden_cell\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n# This function accesses a file in your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef set_hadoop_config_with_credentials_d3bd5b94a9334de59a55a7fed2bedeaa(name):\n    \"\"\"This function sets the Hadoop configuration so it is possible to\n    access data from Bluemix Object Storage V3 using Spark\"\"\"\n\n    prefix = 'fs.swift.service.' + name\n    hconf = sc._jsc.hadoopConfiguration()\n    hconf.set(prefix + '.auth.url', 'https://identity.open.softlayer.com'+'/v3/auth/tokens')\n    hconf.set(prefix + '.auth.endpoint.prefix', 'endpoints')\n    hconf.set(prefix + '.tenant', '6aaf54352357483486ee2d4981f8ef15')\n    hconf.set(prefix + '.username', '1c3a895fa0d04923aec623a25013e8ed')\n    hconf.set(prefix + '.password', 'i33i[1QlOBJ{jvSL')\n    hconf.setInt(prefix + '.http.port', 8080)\n    hconf.set(prefix + '.region', 'dallas')\n    hconf.setBoolean(prefix + '.public', True)\n\n# you can choose any name\nname = 'keystone'\nset_hadoop_config_with_credentials_d3bd5b94a9334de59a55a7fed2bedeaa(name)\n\ndf_data_2 = sqlContext.read.format('com.databricks.spark.csv')\\\n    .options(header='true', inferschema='true')\\\n    .load(\"swift://coursera2.\" + name + \"/PAIRS_DATA_11_19_16T18_45_37.csv\")\ndf_data_2.take(5)\n", "metadata": {"collapsed": false}, "execution_count": 1, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "data": {"text/plain": "[Row(Dataset=u'Daymet', Datalayer=u'Daily maximum temperature', Timestamp=u'12/31/14T00:00:00', Lat=35.15184020996094, Lon=-115.97535705566406, Units=u'C', Value=10.517, DaymetDailymaximumtemperature=10.517, DaymetDailyminimumtemperature=None, DaymetPrecipitationrate=None, DaymetShortwaveradiationdailymean)=None, DaymetVaporpressuredailymean=None),\n Row(Dataset=u'Daymet', Datalayer=u'Daily maximum temperature', Timestamp=u'12/30/14T00:00:00', Lat=35.15184020996094, Lon=-115.97535705566406, Units=u'C', Value=12.053, DaymetDailymaximumtemperature=12.053, DaymetDailyminimumtemperature=None, DaymetPrecipitationrate=None, DaymetShortwaveradiationdailymean)=None, DaymetVaporpressuredailymean=None),\n Row(Dataset=u'Daymet', Datalayer=u'Daily maximum temperature', Timestamp=u'12/29/14T00:00:00', Lat=35.15184020996094, Lon=-115.97535705566406, Units=u'C', Value=11.499, DaymetDailymaximumtemperature=11.499, DaymetDailyminimumtemperature=None, DaymetPrecipitationrate=None, DaymetShortwaveradiationdailymean)=None, DaymetVaporpressuredailymean=None),\n Row(Dataset=u'Daymet', Datalayer=u'Daily maximum temperature', Timestamp=u'12/28/14T00:00:00', Lat=35.15184020996094, Lon=-115.97535705566406, Units=u'C', Value=10.504, DaymetDailymaximumtemperature=10.504, DaymetDailyminimumtemperature=None, DaymetPrecipitationrate=None, DaymetShortwaveradiationdailymean)=None, DaymetVaporpressuredailymean=None),\n Row(Dataset=u'Daymet', Datalayer=u'Daily maximum temperature', Timestamp=u'12/27/14T00:00:00', Lat=35.15184020996094, Lon=-115.97535705566406, Units=u'C', Value=10.499, DaymetDailymaximumtemperature=10.499, DaymetDailyminimumtemperature=None, DaymetPrecipitationrate=None, DaymetShortwaveradiationdailymean)=None, DaymetVaporpressuredailymean=None)]"}, "execution_count": 1}]}, {"source": "df_data_2.printSchema()", "metadata": {"collapsed": false}, "execution_count": 2, "cell_type": "code", "outputs": [{"name": "stdout", "text": "root\n |-- Dataset: string (nullable = true)\n |-- Datalayer: string (nullable = true)\n |-- Timestamp: string (nullable = true)\n |-- Lat: double (nullable = true)\n |-- Lon: double (nullable = true)\n |-- Units: string (nullable = true)\n |-- Value: double (nullable = true)\n |-- DaymetDailymaximumtemperature: double (nullable = true)\n |-- DaymetDailyminimumtemperature: double (nullable = true)\n |-- DaymetPrecipitationrate: double (nullable = true)\n |-- DaymetShortwaveradiationdailymean): double (nullable = true)\n |-- DaymetVaporpressuredailymean: double (nullable = true)\n\n", "output_type": "stream"}]}, {"source": "df_data_2.createOrReplaceTempView(\"df\")\naggregates = sqlContext.sql(\"\"\"\nselect \n    min(DaymetDailymaximumtemperature),max(DaymetDailymaximumtemperature),mean(DaymetDailymaximumtemperature),\n    min(DaymetDailyminimumtemperature),max(DaymetDailyminimumtemperature),mean(DaymetDailyminimumtemperature),\n    min(DaymetPrecipitationrate),max(DaymetPrecipitationrate),mean(DaymetPrecipitationrate) as meanDPR,\n    min(DaymetVaporpressuredailymean),max(DaymetVaporpressuredailymean),mean(DaymetVaporpressuredailymean),\n    cw,\n    cast (concat('20',year) as int) as year\nfrom (\n    select \n        weekofyear(to_date(concat('20',split(split(Timestamp,'T')[0],'/')[2],'-',split(split(Timestamp,'T')[0],'/')[0],'-',split(split(Timestamp,'T')[0],'/')[1],' 00:00:00'))) as cw,\n        split(split(Timestamp,'T')[0],'/')[2] as year,\n        DaymetDailymaximumtemperature,\n        DaymetDailyminimumtemperature,\n        DaymetPrecipitationrate,\n        DaymetVaporpressuredailymean\n        from df\n) group by cw,year\n\"\"\")\n", "metadata": {"collapsed": false}, "execution_count": 31, "cell_type": "code", "outputs": []}, {"source": "from pyspark.sql.types import *\ndf_data_3 = sqlContext.read.format('com.databricks.spark.csv')\\\n    .options(header='true', inferschema='true')\\\n    .load(\"swift://coursera2.\" + name + \"/final.csv\")\ndf_data_3.take(5)\n", "metadata": {"collapsed": false}, "execution_count": 32, "cell_type": "code", "outputs": [{"metadata": {}, "output_type": "execute_result", "data": {"text/plain": "[Row(year=u'1980', CommodityCode=u'268099', CropName=u'ALMOND HULLS ', CountyCode=u'7', County=u'Butte ', HarvestedAcres=u'  ', Yield=u'  ', Production=u'30000', Pricepu=u'59', Unit=u'TONS ', Value=u'1770000'),\n Row(year=u'1980', CommodityCode=u'268099', CropName=u'ALMOND HULLS ', CountyCode=u'11', County=u'Colusa ', HarvestedAcres=u'  ', Yield=u'  ', Production=u'5250', Pricepu=u'60', Unit=u'TONS ', Value=u'315000'),\n Row(year=u'1980', CommodityCode=u'268099', CropName=u'ALMOND HULLS ', CountyCode=u'19', County=u'Fresno ', HarvestedAcres=u'  ', Yield=u'  ', Production=u'23700', Pricepu=u'75', Unit=u'TONS ', Value=u'1778000'),\n Row(year=u'1980', CommodityCode=u'268099', CropName=u'ALMOND HULLS ', CountyCode=u'29', County=u'Kern ', HarvestedAcres=u'  ', Yield=u'  ', Production=u'88600', Pricepu=u'77', Unit=u'TONS ', Value=u'6811000'),\n Row(year=u'1980', CommodityCode=u'268099', CropName=u'ALMOND HULLS ', CountyCode=u'31', County=u'Kings ', HarvestedAcres=u'  ', Yield=u'  ', Production=u'4216', Pricepu=u'50', Unit=u'TONS ', Value=u'210800')]"}, "execution_count": 32}]}, {"source": "joint = aggregates.join(df_data_3,aggregates.year==df_data_3.year)\njoint2 = joint.filter(joint.CountyCode==999).filter(joint.CropName=='WOOL')\\\n    .select('meanDPR','Production')\\\n    .withColumn(\"ProductionTmp\", df_data_3.Production.cast(IntegerType()))\\\n    .drop(\"Production\")\\\n    .withColumnRenamed(\"ProductionTmp\", \"Production\")", "metadata": {"collapsed": false}, "execution_count": 33, "cell_type": "code", "outputs": []}, {"source": "#from pyspark.ml.linalg import Vectors\n#from pyspark.ml.feature import VectorAssembler\n\n#assembler = VectorAssembler(\n#    inputCols=[\"year\", \"Production\" ],\n#    outputCol=\"features\")\n#output = assembler.transform(joint2)\n", "metadata": {"collapsed": false}, "execution_count": 6, "cell_type": "code", "outputs": []}, {"source": "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n\ntrain = joint2.rdd.map(lambda x:LabeledPoint(int(x.Production), [int(x.meanDPR)]))\n", "metadata": {"collapsed": false}, "execution_count": 34, "cell_type": "code", "outputs": []}, {"source": "model = LinearRegressionWithSGD.train(train, iterations=100, step=0.00000001)", "metadata": {"collapsed": false}, "execution_count": 35, "cell_type": "code", "outputs": []}, {"source": "# Evaluate the model on training data\nvaluesAndPreds = train.map(lambda p: (p.label, model.predict(p.features)))\nMSE = valuesAndPreds \\\n    .map(lambda (v, p): (v - p)**2) \\\n    .reduce(lambda x, y: x + y) / valuesAndPreds.count()\nprint(\"Mean Squared Error = \" + str(MSE))", "metadata": {"collapsed": false}, "execution_count": 36, "cell_type": "code", "outputs": [{"name": "stdout", "text": "Mean Squared Error = 5.03402929344e+12\n", "output_type": "stream"}]}, {"source": "", "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": []}], "nbformat": 4}